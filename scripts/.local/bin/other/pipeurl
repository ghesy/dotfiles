#!/bin/bash
# find and open URLs using different programs and methods.
# URLs can be extracted from a string supplied from the command line or standard input.
# if the string is given via stdin, you will be asked to choose one URL using dmenu.
# otherwise, the first URL that is found will be chosen.
# this script detects censorship and bypasses it using proxychains(1).
#
# usage:
#   pipeurl [-c|--clipboard] [open|copy|download|browse|ask|history] URL
#
# dependencies:
#   moreutils, dmenu, libnotify, mpv, sxiv, aria2,
#   gallery-dl, tuir (for opening reddit URLs),
#   my getproxy script (for circumventing censorship) and
#   my ytdl script (for downloading videos).


# =================
# = config
# =================

# a list of download directories. the first accessible one will be chosen.
DL_DIRS=(
    /media/downloads
    ~/Downloads
)

# storage of cached images and files
CACHE_DIR=~/.cache/pipeurl

# =================
# = end of config
# =================

main()
{
    # === global variables
    ACTION=open
    CLIPBOARD=false
    URL=''
    URL_TYPE=''
    readonly WID=${WINDOWID:+-w $WINDOWID}
    readonly NOTIFREP='string:x-canonical-private-synchronous:pipeurl'
    # ===

    for arg; do case $1 in
        -h|--help) print_help; return ;;
        -c|--clipboard) shift; CLIPBOARD=true ;;
        --) shift; break ;;
        -*) echo "${0##*/}: invalid option: $arg" >&2; return 1 ;;
        *) break ;;
    esac; done

    case $1 in
        o|open) shift; ACTION=open ;;
        c|copy) shift; ACTION=copy ;;
        d|dl|download) shift; ACTION=download ;;
        b|browse) shift; ACTION=browse ;;
        a|ask) shift; ACTION='' ;;
        h|history) history_menu; return ;;
    esac

    update_url "$@" || return
    handle_url
}

update_url()
{
    if [[ -n $URL ]]; then
        return 0
    elif [[ -n $1 ]]; then
        URL=$(prnt "$1" | urlgrep | head -1)
    elif [[ $CLIPBOARD = true ]]; then
        URL=$(get_url_from_clipboard) || return
    else
        URL=$(ask_url_stdin) || return
    fi
    [[ -n $URL ]] && return 0 || return 1
}

ask_url_stdin()
{
    urlgrep | sort -u | ifne dmenu $WID -l 10 -p "Select URL${ACTION:+ to $ACTION}"
}

handle_url()
{
    update_url_type
    update_action || return
    case $ACTION in
        copy|browse) ;; *) add_to_history ;;
    esac
    case $URL_TYPE/$ACTION in
        file/open) file_dl_to_cache_and_open ;;
        reddit/tuir|reddit/open) reddit_open ;;
        */stream|video/open) stream_open ;;
        */browse|other/open) exec xdg-open "$URL" ;;
        */aria2|file/download) file_dl_to_dldir ;;
        */ytdl|video/download) stream_dl ;;
        */copy) copy_to_clipboard ;;
        *)
            echo "${0##*/}: invalid url type or action: $URL_TYPE/$ACTION" >&2
            return 1 ;;
    esac
}

add_to_history()
{
    mkdir -pm700 "$CACHE_DIR"
    printf '%s\t%s\n' "$URL_TYPE" "$URL" >> "$CACHE_DIR/history"
}

history_menu()
{
    local sel
    sel=$(column -t -s'	' -o' -- ' "$CACHE_DIR/history" | uniq | tac |
        dmenu $WID -l 10 -p 'PipeURL History') || return
    [[ -z $sel ]] && return 1
    URL=${sel#* -- }
    handle_url
}

get_url_from_clipboard()
{
    local url=$(xclip -o -selection clipboard 2>/dev/null | urlgrep | head -1)
    if [[ -z $url ]]; then
        notify-send -t 1000 PipeURL 'No URLs found in the clipboard.'
        return 1
    else
        notify-send -h "$NOTIFREP" -t 1000 PipeURL 'Found a URL in the clipboard.'
        prnt "$url"
    fi
}

copy_to_clipboard()
{
    prnt "$URL" | xclip -r -selection clipboard
}

reddit_open()
{
    local term sub
    [[ -t 1 ]] && unset term || term="${TERMINAL:?} -e"
    URL=$(get_converted_url)
    sub=$(prnt "$URL" | grep -Po '^(https?://)?[^/]+/r/\K[^/]+(?=/(new|rising|controversial|top|gilded)((/.*)?$)|/?$)') && URL=$sub || unset sub
    exec $term tuir ${sub:+-s} "${URL%/}"
}

stream_dl()
{
    exec setsid -f ${TERMINAL:?} -e ytdl "$URL" >/dev/null 2>&1
}

stream_open()
{
    local msg error=false
    URL=$(get_converted_url)
    msg=$($(getproxy "$URL") mpv --msg-level=all=error -- "$URL" 2>&1)
    case $?/$msg in
        2/*'Temporary failure in name resolution'*)
            proxychains -q mpv --msg-level=all=error -- "$URL" || error=true ;;
        0/*) ;;
        *) error=true; prnt "$msg" ;;
    esac
    if [[ $error = true ]]; then
        msg=${msg#* ERROR: }
        msg=${msg%%(caused*}
        msg=$(prnt "$msg" | head -1)
        notify-send PipeURL "Failed to open the video${msg:+:
$msg}"
    fi
}

file_dl_to_dldir()
{
    get_file_proxy_and_urls || return
    [[ -z $URL ]] && return 1
    prnt "$URL" | while IFS= read -r u; do
        $PXY aria2c -q -x10 -s10 -k1M -m1 --no-conf \
            --async-dns=false -d"$(get_dl_dir)" -- "$u" || {
                notify-send PipeURL "Failed: ${ORIGURL:-$URL}"
                return 1
            }
    done
    notify-send PipeURL "Done: $ORIGURL"
}

file_dl_to_cache_and_open()
{
    ID=$(prnt "$URL" | md5sum | cut -d' ' -f1)
    if [[ ! -d $CACHE_DIR ]] || [[ -z $(find "$CACHE_DIR" -name "$ID-*") ]]; then
        file_dl_to_cache || {
            notify-send PipeURL "Failed: ${ORIGURL:-$URL}"
            rm -- "${CACHE_DIR:?}"/$ID-*
            return 1
        }
    fi
    sxiv -a -- "$CACHE_DIR"/$ID-* || mpv -quiet -- "$CACHE_DIR"/$ID-*
}

file_dl_to_cache()
{
    get_file_proxy_and_urls || return
    [[ -z $URL ]] && return 1
    mkdir -pm700 "$CACHE_DIR" || return
    local n=1
    prnt "$URL" | while IFS= read -r u; do
        $PXY aria2c -q -x10 -s10 -k1M -m1 --no-conf --auto-file-renaming=false \
            --async-dns=false -d"$CACHE_DIR" -o"$ID-$n" -- "$u" || return
         n=$((n+1))
    done
}

get_file_proxy_and_urls()
{
    ORIGURL=$URL
    PXY=$(getproxy "$URL")
    # if it's a gallery or image site, get the raw URL(s) using gallery-dl.
    case $(get_simple_url) in
        *.jpg|*.jpeg|*.png|*.webp|*.gif) ;;
        reddit.com/gallery/*|i.redd.it*|imgur.com/*|gfycat.com/*)
            URL=$($PXY gallery-dl -G -- "$URL") || return
            PXY=$(getproxy "$(prnt "$URL" | head -1)")
        ;;
    esac
}

update_url_type()
{
    [[ -n $URL_TYPE ]] && return 0
    case $(get_simple_url) in
        *.pdf|*.cbz|*.cbr|*.jpg|*.jpeg|*.png|*.webp|*.gif)
            URL_TYPE=file
        ;;
        *.mp4|*.mp4?*|*.webm|*.gifv|*.mkv|*.mp3|*.mp3?*|*.flac|*.opus|*.ogg|*.m3u8|youtube.com/watch*|\
        youtube.com/v/*|youtube.com/playlist*|youtube.com/embed/*|youtube.com/shorts/*|youtu.be/*|\
        hooktube.com/watch*|bitchute.com/video/*|lbry.tv/@*/*|odysee.com/@*/*|odysee.com/*:$(rep 40 '?')*|\
        khanacademy.org/*/v/*|v.redd.it/*|tiktok.com/@*/video/*|aparat.com/v/*|aparat.com/*/live|\
        streamff.com/v/*|mixture.gg/v/*|streamgg.com/v/*|streamwo.com/v/*|clippituser.tv/c/*|\
        streamable.com/*|streamja.com/*|piped.*/watch*|invidious.*/watch*|v.fodder.gg/v/*)
            URL_TYPE=video
        ;;
        reddit.com/gallery/*|i.redd.it*|imgur.com/*|gfycat.com/*)
            URL_TYPE=file
        ;;
        reddit.com/*|redd.it/*|libreddit.*/*)
            URL_TYPE=reddit
        ;;
        */w/*|*/videos/watch/*|*/videos/embed/*) # detect peertube URLs
            local u='[0-9a-fA-F]' # match a UID character
            local ptrn="^[^/]+/(videos/(watch|embed)|w(/p)?)/([0-9a-zA-Z]{22}|$u{8}(-$u{4}){3}-$u{12})($|(\?|#)\S+)"
            get_simple_url | grep -Pq "$ptrn" && URL_TYPE=video
        ;;
        *) URL_TYPE=other ;;
    esac
}

update_action()
{
    [[ -n $ACTION ]] && return 0
    local actlist action
    case $URL_TYPE in
        file)   actlist='Open\naria2\nCopy\nBrowse\nytdl\n' ;;
        reddit) actlist='tuir\nStream\nytdl\nCopy\nBrowse\n' ;;
        video)  actlist='Stream\nytdl\nCopy\nBrowse\n' ;;
        other)  actlist='Browse\nStream\nCopy\nytdl\naria2\n' ;;
    esac
    action=$(printf "$actlist" | dmenu $WID -p 'Choose Action') || return
    case $action in
        Open|Stream|Copy|Browse|ytdl|aria2|tuir) ACTION=${action,,} ;;
        *)
            echo "${0##*/}: invalid action: $action" >&2
            return 1 ;;
    esac
}

urlgrep()
{
    local domainchar='[a-zA-Z\d\.-]'
    local urlchar='[a-zA-Z0-9\.,:;!\?'\''\\\+\*&%\$#=~_@()-]'
    grep -Po "(ht|f)tps?://(www\.)?$domainchar+(:\d+(?=(/|$)))?(/$urlchar+)*" | sed "s/[\)',.]$//"
}

get_simple_url()
{
    local site=${URL#*://}; site=${site#www.} site=${site%%'?'*} site=${site%/}
    prnt "$site"
}

get_url_path()
{
    local path=${URL#*://}; path=${path#*/}
    prnt "$path"
}

get_converted_url()
{
    local simple=$(get_simple_url) path=$(get_url_path)
    case $simple in
        piped.*/watch*) prnt "https://www.youtube.com/$path" ;;
        libreddit.*/*) prnt "https://www.reddit.com/$path" ;;
        *) prnt "$URL" ;;
    esac
}

get_dl_dir()
{
    for dir in "${DL_DIRS[@]}"; do
        [[ -d $dir ]] && [[ -r $dir ]] && [[ -w $dir ]] || continue
        prnt "$dir"
        return
    done
    echo ~
}

rep()
{
    printf "$2%.0s" $(eval echo "{1..$1}")
}

prnt()
{
    printf '%s\n' "$@"
}

print_help()
{
    cat << 'EOF'
usage:
  pipeurl [-c|--clipboard] [open|copy|download|browse|ask|history] URL

options:
  -c, --clipboard  get the url from the clipboard
  -h, --help       print this help message and exit

the first parameter after the options is the
action that will be performed on the URL.
EOF
}

main "$@"
