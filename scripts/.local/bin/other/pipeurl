#!/bin/bash
# find and open URLs using different programs and methods.
# URLs can be extracted from a string supplied from the command line or standard input.
# if the string is given via stdin, you will be asked to choose one URL using dmenu.
# otherwise, the first URL that is found will be chosen.
# this script detects censorship and bypasses it using proxychains(1).
#
# options:
#   no options:       open the URL (ex. open videos using mpv).
#   -c, --copy        copy the URL to clipboard and exit.
#   -d, --download    download the contents of the URL instead of opening it.
#                     (ex. videos will be passed to my ytdl script to be downloaded).
#   -l, --listen      listen to the clipboard for one minute and use the first URL that is copied there.
#   -a, --ask         ask wether to open, download or copy the URL using dmenu.
#   -H, --history     select and open an item from the history using dmenu.
#
# usage:
#   pipeurl [--listen] [--copy|--download|--ask] [TEXT_CONTAINING_URL]
#
# dependencies:
#   moreutils, dmenu, libnotify, mpv, aria2,
#   gallery-dl, tuir (for opening reddit URLs),
#   my getproxy script (for circumventing censorship) and
#   my ytdl script (for downloading videos).
#
# TODO: write the print_help() function


# =================
# = config
# =================

# a list of download directories. the first accessible one will be chosen.
DL_DIRS="
/media/downloads
$HOME/Downloads
"

# storage of cached images and files
CACHE_DIR=~/.cache/pipeurl

# =================
# = end of config
# =================

set -E -o functrace
failure() { local lineno=$1; local msg=$2; echo "Failed at $lineno: $msg" ;}
trap 'failure $LINENO "$BASH_COMMAND"' ERR

# note: variables that are used globally (by multiple functions) are in capitol letters.

main()
{
    # global variables
    ACTION=open
    CLIPBOARD=false
    URL=''
    URL_TYPE=''
    readonly WID=${WINDOWID:+-w $WINDOWID}
    readonly REP='string:x-canonical-private-synchronous:pipeurl'

    for arg; do case $1 in
        -h|--help) print_help; return ;;
        -c|--clipboard) shift; CLIPBOARD=true ;;
        --urlgrep) urlgrep; return ;; # for internal use
        --) shift; break ;;
        -*) echo "${0##*/}: invalid option: $arg"; return ;;
        *) break ;;
    esac; done

    case $1 in
        c|copy) shift; ACTION=copy ;;
        d|dl|download) shift; ACTION=download ;;
        b|browse) shift; ACTION=browse ;;
        a|ask) shift; ACTION='' ;;
        h|history) history_menu; return ;;
    esac

    update_url "$@" || return
    handle_url
}

update_url()
{
    if [ -n "$URL" ]; then
        return 0
    elif [ -n "$1" ]; then
        URL=$(prnt "$1" | urlgrep | head -1)
    elif [ "$CLIPBOARD" = true ]; then
        URL=$(get_url_from_clipboard) || return
    else
        URL=$(ask_url_stdin) || return
    fi
    [ -n "$URL" ] && return 0 || return 1
}

ask_url_stdin()
{
    urlgrep | sort -u | ifne dmenu $WID -l 30 -p "Select URL${ACTION:+ to $ACTION}"
}

handle_url()
{
    update_url_type
    update_action || return
    [ "$ACTION" = copy ] && { copy_to_clipboard; return ;}
    printf '%s\t%s\n' "$URL_TYPE" "$URL" >> "$CACHE_DIR/history"
    case $URL_TYPE/$ACTION in
        file/open) file_dl_to_cache_and_open ;;
        file/download) file_dl_to_dldir ;;
        stream/open) stream_open ;;
        stream/download) stream_dl ;;
        reddit/open) reddit_open ;;
        other/open) exec xdg-open "$URL" ;;
        */browse) exec ${BROWSER:-xdg-open} "$URL" ;;
        *) echo "the specified action is invalid for this URL.: $ACTION" >&2 ;;
    esac
}

history_menu()
{
    local sel
    sel=$(column -t -s'	' -o' -- ' "$CACHE_DIR/history" | uniq | tac |
        dmenu $WID -l 30 -p 'PipeURL History') || return
    [ -z "$sel" ] && return 1
    URL=${sel#* -- }
    handle_url
}

get_url_from_clipboard()
{
    local pids=$(pgrep timeout -P"$(pgrep -d, "${0##*/}")")
    if [ -n "$pids" ]; then
        kill $pids
        notify-send -h $REP -t 1000 -u low PipeURL 'Stopped Listening to Clipboard.'
        return 1
    fi
    notify-send -h $REP -t 1000 -u low PipeURL 'Listening to Clipboard...'
    scriptpath=$0 rep=$REP timeout 1m sh << 'eof'
        while :; do
            xclip -selection clipboard </dev/null
            clipnotify || exit
            clip=$(xclip -o -selection clipboard)
            [ -z "$clip" ] && continue
            url=$(printf '%s\n' "$clip" | "$scriptpath" --urlgrep | head -1)
            [ -z "$url" ] && continue
            notify-send -h $rep -t 1000 -u low PipeURL 'Got it.'
            printf '%s\n' "$url"
            break
        done
eof
}

copy_to_clipboard()
{
    prnt "$URL" | xclip -r -selection clipboard
}

reddit_open()
{
    local term
    [ -t 1 ] && unset term || term="${TERMINAL:?} -e"
    exec $term tuir $(prnt "$URL" | sed -E 's|.*/r/([^/]+)/?$|-s \1|')
}

stream_dl()
{
    exec setsid -f ${TERMINAL:?} -e ytdl "$URL" >/dev/null 2>&1
}

stream_open()
{
    local msg error=false
    notify-send -h $REP -u low -t 1000 PipeURL 'Opening video...'
    msg=$($(getproxy "$URL") mpv --msg-level=all=error -- "$URL" 2>&1)
    case $?/$msg in
        2/*'Temporary failure in name resolution'*)
            proxychains -q mpv --msg-level=all=error -- "$URL" || error=true ;;
        0/*) ;;
        *) error=true; prnt "$msg" ;;
    esac
    if [ "$error" = true ]; then
        msg=${msg#* ERROR: }
        msg=${msg%%(caused*}
        msg=$(prnt "$msg" | head -1)
        notify-send PipeURL "Failed to open the video${msg:+:
$msg}"
    fi
}

file_dl_to_dldir()
{
    get_file_proxy_and_urls || return
    [ -z "$URL" ] && return 1
    prnt "$URL" | while IFS= read -r u; do
        $PXY aria2c -q -x10 -s10 -k1M -m1 --no-conf \
            --async-dns=false -d"$(get_dl_dir)" -- "$u" || {
                notify-send PipeURL "Failed: ${ORIGURL:-$URL}"
                return 1
            }
    done
    notify-send PipeURL "Done: $ORIGURL"
}

file_dl_to_cache_and_open()
{
    ID=$(prnt "$URL" | md5sum | cut -d' ' -f1)
    if [ ! -d "$CACHE_DIR" ] || [ -z "$(find "$CACHE_DIR" -name "$ID-*")" ]; then
        file_dl_to_cache || {
            notify-send PipeURL "Failed: ${ORIGURL:-$URL}"
            rm -- "${CACHE_DIR:?}"/$ID-*
            return 1
        }
    fi
    sxiv -a -- "$CACHE_DIR"/$ID-* || mpv -quiet -- "$CACHE_DIR"/$ID-*
}

file_dl_to_cache()
{
    get_file_proxy_and_urls || return
    [ -z "$URL" ] && return 1
    local n=1
    prnt "$URL" | while IFS= read -r u; do
        $PXY aria2c -q -x10 -s10 -k1M -m1 --no-conf --auto-file-renaming=false \
            --async-dns=false -d"$CACHE_DIR" -o"$ID-$n" -- "$u" || return
         n=$((n+1))
    done
}

get_file_proxy_and_urls()
{
    ORIGURL=$URL
    PXY=$(getproxy "$URL")
    # if it's a gallery or image site, get the raw URL(s) using gallery-dl.
    case $(get_simple_url) in
        *.jpg|*.jpeg|*.png|*.webp|*.gif) ;;
        reddit.com/gallery/*|i.redd.it*|imgur.com/*|gfycat.com/*)
            URL=$($PXY gallery-dl -G -- "$URL") || return
            PXY=$(getproxy "$(prnt "$URL" | head -1)")
        ;;
    esac
}

update_url_type()
{
    case $(get_simple_url) in
        *.pdf|*.cbz|*.cbr|*.jpg|*.jpeg|*.png|*.webp|*.gif)
            URL_TYPE=file
        ;;
        reddit.com/gallery/*|i.redd.it*|imgur.com/*|gfycat.com/*)
            URL_TYPE=file
        ;;
        *.mp4|*.mp4?*|*.webm|*.gifv|*.mkv|*.mp3|*.mp3?*|*.flac|*.opus|*.ogg|*.m3u8|youtube.com/watch*|\
        youtube.com/v/*|youtube.com/playlist*|youtube.com/embed/*|youtube.com/shorts/*|youtu.be/*|\
        hooktube.com/watch*|bitchute.com/video/*|lbry.tv/@*/*|odysee.com/@*/*|odysee.com/*:$(rep 40 '?')*|\
        khanacademy.org/*/v/*|v.redd.it/*|tiktok.com/@*/video/*|aparat.com/v/*|aparat.com/*/live|\
        streamff.com/v/*|mixture.gg/v/*|streamgg.com/v/*|streamwo.com/v/*|clippituser.tv/c/*|\
        streamable.com/*|streamja.com/*|piped.kavin.rocks/watch*)
            URL_TYPE=stream
        ;;
        reddit.com*|redd.it*)
            URL_TYPE=reddit
        ;;
        */w/*|*/videos/watch/*|*/videos/embed/*) # detect peertube URLs
            local u='[0-9a-fA-F]' # match a UID character
            local ptrn="^[^/]+/(videos/(watch|embed)|w(/p)?)/([0-9a-zA-Z]{22}|$u{8}(-$u{4}){3}-$u{12})($|(\?|#)\S+)"
            get_simple_url | grep -Pq "$ptrn" && URL_TYPE=stream
        ;;
        *) URL_TYPE=other ;;
    esac
}

update_action()
{
    [ -n "$ACTION" ] && return 0
    local action actions='Open\nDownload\nCopy\nBrowse\n'
    case $URL_TYPE in
        reddit) actions='Open\nCopy\nBrowse\n' ;;
        other)  actions='Copy\nBrowse\n' ;;
    esac
    action=$(printf "$actions" | dmenu $WID -p 'Choose Action') || return
    case $action in
        Open|Copy|Download|Browse) ACTION=${action,,} ;;
        *)
            echo "the specified action doesn't exist." >&2
            return 1 ;;
    esac
}

urlgrep()
{
    local domainchar='[a-zA-Z\d\.-]'
    local urlchar='[a-zA-Z0-9\.,:;!\?'\''\\\+\*&%\$#=~_@()-]'
    grep -Po "(ht|f)tps?://(www\.)?$domainchar+(:\d+(?=(/|$)))?(/$urlchar+)*" | sed "s/[\)']$//"
}

get_simple_url()
{
    local site=${URL#*://}; site=${site#www.} site=${site%%'?'*} site=${site%/}
    prnt "$site"
}

get_dl_dir()
{
    prnt "$DL_DIRS" | while IFS= read -r dir; do
        [ -d "$dir" ] && [ -r "$dir" ] && [ -w "$dir" ] || continue
        prnt "$dir"
        return
    done
    echo ~
}

rep()
{
    printf "$2%.0s" $(eval echo "{1..$1}")
}

prnt()
{
    printf '%s\n' "$@"
}

main "$@"
